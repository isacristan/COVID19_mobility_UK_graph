{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import numpy as np\n",
    "from geopandas.tools import sjoin\n",
    "import shapely\n",
    "from shapely.geometry import Point\n",
    "import pyreadr\n",
    "import mercantile\n",
    "from shapely.geometry import shape\n",
    "import networkx as nx\n",
    "import seaborn as sn\n",
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "from scipy.optimize import least_squares\n",
    "import datetime\n",
    "import lmfit\n",
    "from sklearn import metrics\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of timeseries of networks for daily mobility flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "root_data = '/Users/ignaciosacristanbarba/Documents/M4R/Data'\n",
    "\n",
    "# Reads in the timeseries of between tiles movement data\n",
    "root1 = root_data+'/BETWEEN_TILES_TRIP_NUMBERS.csv'\n",
    "df = pd.read_csv(root1)\n",
    "\n",
    "\n",
    "# Load LSCC \n",
    "root_results = '/Users/ignaciosacristanbarba/Documents/M4R/Results' \n",
    "root_Base = root_results+'/Base Network/'\n",
    "root2 = root_Base+'/base_network_lscc.npz'\n",
    "\n",
    "with open(root2, 'rb') as handle:\n",
    "    lscc_dict = pickle.load(handle)\n",
    "        \n",
    "# Generate DiGraph from data \n",
    "lscc = nx.from_dict_of_dicts(lscc_dict,create_using = nx.DiGraph)\n",
    "lscc_nodes = list(lscc.nodes())\n",
    "n_nodes = len(lscc_nodes)\n",
    "node_dict = {lscc_nodes[i] : i for i in range(n_nodes)}\n",
    "\n",
    "\n",
    "# Compute adjacency matrix and node list of LSCC\n",
    "A_LSCC = nx.adjacency_matrix(lscc)\n",
    "\n",
    "########################\n",
    "# Load D_geom for LSCC #\n",
    "########################\n",
    "\n",
    "root_D_geom = root_Base + '/lscc_D_geom.pickle'\n",
    "with open(root_D_geom, 'rb') as handle:\n",
    "        D_geom = pickle.load(handle)\n",
    "        \n",
    "        \n",
    "##############################\n",
    "# Load D_geom for Full Graph #\n",
    "##############################\n",
    "\n",
    "root_D_geom_full = root_Base + '/base_D_geom.pickle'\n",
    "with open(root_D_geom_full, 'rb') as handle:\n",
    "        D_geom_full = pickle.load(handle)\n",
    "\n",
    "###########################\n",
    "# Get dates of timeseries #\n",
    "###########################\n",
    "\n",
    "# Get start and end dates\n",
    "start_date = df.columns.values[2][:10]\n",
    "end_date = df.columns.values[-1][:10]\n",
    "\n",
    "# Generate DatetimeIndex\n",
    "days = pd.date_range(start=start_date, end=end_date).date\n",
    "n_days = len(days)\n",
    "days_dm = np.asarray([str(days[i])[5:] for i in range(n_days)])\n",
    "days_week = np.asarray(pd.date_range(start=start_date, end=end_date).weekofyear, dtype='int')\n",
    "weeks = np.arange(days_week.min(),days_week.max())\n",
    "\n",
    "# Indicate weekdays\n",
    "weekday = pd.date_range(start=start_date, end=end_date).weekday<5\n",
    "\n",
    "# Consider bank holidays\n",
    "bank_holidays = []\n",
    "bank_holidays.append(np.argwhere(days==datetime.date(2020, 4, 10))[0][0])\n",
    "bank_holidays.append(np.argwhere(days==datetime.date(2020, 4, 13))[0][0])\n",
    "bank_holidays.append(np.argwhere(days==datetime.date(2020, 5, 8))[0][0])\n",
    "bank_holidays.append(np.argwhere(days==datetime.date(2020, 5, 25))[0][0])\n",
    "weekday[bank_holidays] = False\n",
    "\n",
    "# Store lockdown-date\n",
    "lockdown_date = pd.to_datetime('20200324', format='%Y%m%d', errors='ignore')\n",
    "lockdown_date_number = np.argwhere(days == lockdown_date).flatten()[0]\n",
    "lockdown_week = lockdown_date.week\n",
    "\n",
    "# Store date information\n",
    "timestamps = {'days' : days, 'weekdays' : weekday, 'weeks' : weeks, \n",
    "              'lockdown_day' : lockdown_date}\n",
    "\n",
    "root_store = '/Users/ignaciosacristanbarba/Documents/M4R/Results'+'/Timeseries/'\n",
    "root3 = root_store+'timestamps.pkl'\n",
    "\n",
    "# Store data (serialize)\n",
    "with open(root3, 'wb') as handle:\n",
    "    pickle.dump(timestamps, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NUTS shape files\n",
    "root = '/Users/ignaciosacristanbarba/Documents/M4R/Data'\n",
    "root_map = root+'/NUTS_Level_3__January_2018__Boundaries-shp/NUTS_Level_3__January_2018__Boundaries.shp'\n",
    "map_gdf = gpd.read_file(root_map)\n",
    "map_gdf = map_gdf.to_crs(\"EPSG:3395\")\n",
    "\n",
    "# Get NUTS3 \n",
    "cols = [0,2,3,4,5,6,7,8]\n",
    "gdf_NUTS3 = map_gdf.drop(map_gdf.columns[cols],axis=1)\n",
    "gdf_NUTS3.rename(columns={'nuts318cd': 'nuts'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of daily evolution of LSCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate network for each day by summing over all hours per day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewire_graph(G):\n",
    "    '''\n",
    "    Permute nodes in datastructure such that G.nodes() == lscc.nodes()\n",
    "    '''\n",
    "    n_nodes = len(G.nodes())\n",
    "    G_dict = nx.to_dict_of_dicts(G)\n",
    "    new_dict = {list(lscc_dict.keys())[i] : G_dict[list(lscc_dict.keys())[i]] for i in range(n_nodes)}\n",
    "    G_new = nx.from_dict_of_dicts(new_dict, create_using = nx.DiGraph)\n",
    "    \n",
    "    return G_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_again = False\n",
    "\n",
    "if compute_again == True:\n",
    "    \n",
    "    # Converting the quadkeys to strings\n",
    "    df['start_quadkey'] = df['start_quadkey'].astype(str)\n",
    "    df['end_quadkey'] = df['end_quadkey'].astype(str)\n",
    "    # adding a leading '0' to quadkeys beginning with 3 so it maps on to web mercator\n",
    "    df.loc[df['start_quadkey'].str[:1] == '3', 'start_quadkey'] = '0'+df['start_quadkey']\n",
    "    df.loc[df['end_quadkey'].str[:1] == '3', 'end_quadkey'] = '0'+df['end_quadkey']\n",
    "\n",
    "    # Replace nan by 0\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    ###################################\n",
    "    # Filtering out rows outside LSCC #\n",
    "    ###################################\n",
    "\n",
    "    # Get LSCC quadkeys\n",
    "    quadkeys_timeseries_LSCC = set(lscc_dict.keys())\n",
    "\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "\n",
    "        start_included =  df.iloc[i]['start_quadkey'] in quadkeys_timeseries_LSCC\n",
    "        end_included = df.iloc[i]['end_quadkey'] in quadkeys_timeseries_LSCC \n",
    "        included = start_included + end_included\n",
    "\n",
    "        if included != 2:\n",
    "            df_filtered = df_filtered.drop([i])\n",
    "\n",
    "    print('Number of deleted rows:', df.shape[0] - df_filtered.shape[0])\n",
    "\n",
    "    df = df_filtered.reset_index(drop=True)\n",
    "    \n",
    "    #####################################\n",
    "    # Generate daily DiGraphs from data #\n",
    "    #####################################\n",
    "\n",
    "    # Get the first two quadkey columns\n",
    "    df_key = df.iloc[:,:2]\n",
    "\n",
    "    M = df.shape[1]-3\n",
    "    networks = []\n",
    "\n",
    "    for i in range(1,M,3):\n",
    "\n",
    "        # Get daily data \n",
    "        df_mov = df.iloc[:,i+1:i+4]\n",
    "        df_sum = pd.Series(df_mov.sum(axis=1),name='movement')\n",
    "        df_sum = pd.concat([df_key,df_sum],axis=1)\n",
    "\n",
    "        # Generate DiGraph \n",
    "        N = len(df_sum)\n",
    "        G = nx.DiGraph()\n",
    "        # for each row, add nodes and weighted edge\n",
    "        for j in range(0,N):\n",
    "            start = df_sum['start_quadkey'][j]\n",
    "            end = df_sum['end_quadkey'][j]\n",
    "            weight = df_sum['movement'][j]\n",
    "            G.add_node(start)\n",
    "            G.add_node(end)\n",
    "            if weight > 0.0:\n",
    "                G.add_weighted_edges_from([(start, end, weight)] )\n",
    "\n",
    "        # Append DiGraph to list of networks\n",
    "        networks.append(G)\n",
    "\n",
    "    ###########################################\n",
    "    # Permute nodes such that they match LSCC #\n",
    "    ###########################################\n",
    "\n",
    "    for i in range(len(networks)):\n",
    "        networks[i] = rewire_graph(networks[i])\n",
    "        \n",
    "    ########################\n",
    "    # Store daily DiGraphs #\n",
    "    ########################\n",
    "\n",
    "    root4 = root_store+'timeseries_daily_digraphs_lscc.pkl'\n",
    "\n",
    "    # Store data (serialize)\n",
    "    with open(root4, 'wb') as handle:\n",
    "        pickle.dump(networks, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "# Load networks      \n",
    "else: \n",
    "    \n",
    "    root4 = root_store+'timeseries_daily_digraphs_lscc.pkl'\n",
    "    with open(root4, 'rb') as handle:\n",
    "        networks = pickle.load(handle)\n",
    "        \n",
    "##############################\n",
    "# Compute adjacency matrices #\n",
    "##############################\n",
    "\n",
    "n_networks = len(networks)\n",
    "networks_adjacency = [nx.adjacency_matrix(networks[i]) for i in range(n_networks)]\n",
    "        \n",
    "\n",
    "#####################\n",
    "# Compute geography #\n",
    "#####################\n",
    "\n",
    "# add polygons as node attributes\n",
    "G = networks[0]\n",
    "quadkeys = list(G.nodes)\n",
    "n_nodes = len(quadkeys)\n",
    "\n",
    "polys = []\n",
    "\n",
    "#Iterates over the quadkeys to extract the tiles\n",
    "for quadkey in quadkeys:\n",
    "    tile = mercantile.feature(mercantile.quadkey_to_tile(quadkey), projected = 'web mercator')\n",
    "    polys.append(tile.get('geometry'))\n",
    "\n",
    "geom = [shape(i) for i in polys]\n",
    "geom_dict = {quadkeys[i] : list(geom[i].centroid.bounds[:2]) for i in range(0,n_nodes)}\n",
    "\n",
    "n_networks = len(networks)\n",
    "\n",
    "for i in range(0,n_days):\n",
    "    G = networks[i]\n",
    "    nx.set_node_attributes(G,geom_dict,'geom')\n",
    "\n",
    "# Store node keys\n",
    "node_numbers = {i : list(networks[0].nodes())[i] for i in range(n_nodes)}\n",
    "# Gemo dicy for node keys\n",
    "geom_dict_numbers = {i : geom_dict[node_numbers[i]] for i in range(n_nodes)}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3125"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(node_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All networks have the same nodes and the same node keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_figure = '/Users/ignaciosacristanbarba/Documents/M4R/Figures/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute evolution of radius of gyration for LSCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2_total = (D_geom + np.diag(2.5*np.ones(n_nodes)))**2\n",
    "D2_inter  = (D_geom )**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_total_nodal = np.zeros((n_days, n_nodes))\n",
    "r_total_median = []\n",
    "r_total_Q1 = []\n",
    "r_total_Q3 = []\n",
    "r_inter_nodal = np.zeros((n_days, n_nodes))\n",
    "r_inter_median = []\n",
    "r_inter_Q1 = []\n",
    "r_inter_Q3 = []\n",
    "\n",
    "for i, A in enumerate(networks_adjacency):\n",
    "    # Get adjacency matrix\n",
    "    A = A.toarray()\n",
    "    # Compute out strengths\n",
    "    d_out = np.sum(A,axis=1)\n",
    "    # Compute radius of Gyration with self-loops\n",
    "    r = np.sqrt(np.sum(A/d_out*D2_total, axis = 1))\n",
    "    r_total_nodal[i,:] = r\n",
    "    # Without self-loops\n",
    "    r = np.sqrt(np.sum(A/d_out*D2_inter, axis = 1))\n",
    "    r_inter_nodal[i,:] = r\n",
    "    \n",
    "r_total_median = np.median(r_total_nodal,axis=1)\n",
    "r_total_Q1 = np.percentile(r_total_nodal,25,axis=1)\n",
    "r_total_Q3 = np.percentile(r_total_nodal,75,axis=1)\n",
    "r_inter_median = np.median(r_inter_nodal,axis=1)\n",
    "r_inter_Q1 = np.percentile(r_inter_nodal,25,axis=1)\n",
    "r_inter_Q3 = np.percentile(r_inter_nodal,75,axis=1)\n",
    "\n",
    "# Store results in dataframe\n",
    "results = pd.DataFrame({'day':days})\n",
    "results['r_total_median'] = r_total_median\n",
    "results['r_total_Q1'] = r_total_Q1\n",
    "results['r_total_Q3'] = r_total_Q3\n",
    "results['r_inter_median'] = r_inter_median\n",
    "results['r_inter_Q1'] = r_inter_Q1\n",
    "results['r_inter_Q3'] = r_inter_Q3 \n",
    "    \n",
    "# Compute baseline values\n",
    "A = A_LSCC.toarray()\n",
    "d_out = np.sum(A,axis=1)\n",
    "r_total_nodal_lscc = np.sqrt(np.sum(A/d_out*D2_total, axis = 1))\n",
    "r_total_median_lscc = np.percentile(r_total_nodal_lscc,50)\n",
    "r_total_Q1_lscc = np.percentile(r_total_nodal_lscc,25)\n",
    "r_total_Q3_lscc = np.percentile(r_total_nodal_lscc,75)\n",
    "r_inter_nodal_lscc = np.sum(A/d_out*D2_inter, axis = 1)\n",
    "r_inter_median_lscc = np.percentile(r_inter_nodal_lscc,50)\n",
    "r_inter_Q1_lscc = np.percentile(r_inter_nodal_lscc,25)\n",
    "r_inter_Q3_lscc = np.percentile(r_inter_nodal_lscc,75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save r_total_nodal\n",
    "root4 = root_results + '/Timeseries' +'/radius_of_gyration_lscc.csv' \n",
    "col_names = [node_numbers[i] for i in range(n_nodes)]\n",
    "r_total_nodal_df = pd.DataFrame(r_total_nodal,columns = col_names).transpose()\n",
    "r_total_nodal_df.to_csv(root4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
