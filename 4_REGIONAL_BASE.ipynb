{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import numpy as np\n",
    "from geopandas.tools import sjoin\n",
    "import shapely\n",
    "from shapely.geometry import Point\n",
    "import pyreadr\n",
    "import mercantile\n",
    "from shapely.geometry import shape\n",
    "import networkx as nx\n",
    "import seaborn as sn\n",
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "from scipy.optimize import least_squares\n",
    "import datetime\n",
    "import lmfit\n",
    "from sklearn import metrics\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of NUTS3 data: (5436, 180)\n",
      "The max is obtained in average with: 0.93\n",
      "NUTS3 communities in LSCC: 170\n",
      "NUTS2 communities in LSCC: 42\n",
      "NUTS1 communities in LSCC: 12\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# Load datasets #\n",
    "#################\n",
    "\n",
    "root_data = '/Users/ignaciosacristanbarba/Documents/M4R/Data'\n",
    "root_results = '/Users/ignaciosacristanbarba/Documents/M4R/Results'\n",
    "\n",
    "root_base = root_results+'/Base Network'\n",
    "\n",
    "# Load LSCC\n",
    "root_lscc = root_base+'/base_network_lscc.npz'\n",
    "with open(root_lscc, 'rb') as handle:\n",
    "        lscc_dict = pickle.load(handle)\n",
    "        \n",
    "##############################\n",
    "# Generate DiGraph from data #\n",
    "##############################\n",
    "\n",
    "lscc = nx.from_dict_of_dicts(lscc_dict,create_using = nx.DiGraph)\n",
    "lscc_nodes = list(lscc.nodes())\n",
    "n_nodes = len(lscc_nodes)\n",
    "\n",
    "# Store node numbers\n",
    "node_numbers = {i : lscc_nodes[i] for i in range(n_nodes)}\n",
    "\n",
    "# Load geom_dict\n",
    "root_geom_dict = root_base+'/base_geom_dict.pickle'\n",
    "with open(root_geom_dict, 'rb') as handle:\n",
    "        geom_dict = pickle.load(handle)\n",
    "# Gemo dict for node keys\n",
    "geom_dict_numbers = {i : geom_dict[node_numbers[i]] for i in range(n_nodes)}\n",
    "\n",
    "# Compute adjacency matrix and node list of LSCC\n",
    "A_LSCC = nx.adjacency_matrix(lscc)\n",
    "A_LSCC_array = A_LSCC.toarray()\n",
    "lscc_nodes_list = np.asarray(list(lscc.nodes()))\n",
    "s_lscc = len(lscc_nodes_list)\n",
    "\n",
    "# Subtract self-loops\n",
    "B_LSCC_array = A_LSCC - np.diag(np.diag(A_LSCC.toarray()))\n",
    "B_LSCC = sp.csr_matrix(B_LSCC_array)\n",
    "\n",
    "# Import NUTS shape files\n",
    "root_data = '/Users/ignaciosacristanbarba/Documents/M4R/Data/'\n",
    "root_map = root_data+'/NUTS_Level_3__January_2018__Boundaries-shp/NUTS_Level_3__January_2018__Boundaries.shp'\n",
    "map_gdf = gpd.read_file(root_map)\n",
    "map_gdf = map_gdf.to_crs(\"EPSG:3395\")\n",
    "\n",
    "# Get NUTS3 \n",
    "cols = [0,2,3,4,5,6,7,8]\n",
    "gdf_NUTS3 = map_gdf.drop(map_gdf.columns[cols],axis=1)\n",
    "gdf_NUTS3.rename(columns={'nuts318cd': 'nuts'}, inplace=True)\n",
    "\n",
    "# Get NUTS2\n",
    "gdf_NUTS2 = gdf_NUTS3.copy()\n",
    "gdf_NUTS2['nuts'] = [gdf_NUTS2['nuts'][i][:4] for i in range(179)]\n",
    "gdf_NUTS2 = gdf_NUTS2.dissolve(by='nuts')\n",
    "\n",
    "# Get NUTS1\n",
    "gdf_NUTS1 = gdf_NUTS3.copy()\n",
    "gdf_NUTS1['nuts'] = [gdf_NUTS1['nuts'][i][:3] for i in range(179)]\n",
    "gdf_NUTS1 = gdf_NUTS1.dissolve(by='nuts')\n",
    "\n",
    "# Load NUTS3 data\n",
    "root_results = '/Users/ignaciosacristanbarba/Documents/M4R/Results/'\n",
    "root_NUTS3_base = root_results+'/NUTS1_Base/'+'MOVEMENT_QUADKEY_NUTS3_GB.csv'\n",
    "df_NUTS = pd.read_csv(root_NUTS3_base)\n",
    "print('Shape of NUTS3 data:', df_NUTS.shape)\n",
    "\n",
    "# Delete _ in front of quadkeys\n",
    "quadkeys = [df_NUTS['quadkey'][i][1:] for i in range(df_NUTS.shape[0])]\n",
    "\n",
    "# Get NUTS3 region names\n",
    "NUTS3 = df_NUTS.columns.values.tolist()[1:]\n",
    "\n",
    "# Get NUTS2 regions\n",
    "to_NUTS2 = {NUTS3[i] : NUTS3[i][:4] for i in range(len(NUTS3))}\n",
    "NUTS2_index = {list(to_NUTS2.values())[i] : i for i in range(len(to_NUTS2))}\n",
    "\n",
    "# Get NUTS1 regions\n",
    "to_NUTS1 = {NUTS3[i] : NUTS3[i][:3] for i in range(len(NUTS3))}\n",
    "NUTS1_index = {list(to_NUTS1.values())[i] : i for i in range(len(to_NUTS1))}\n",
    "\n",
    "# Compute for each quadkey NUTS3 region by max vote \n",
    "X = np.asarray(df_NUTS.iloc[:,1:])\n",
    "max_rule = np.argmax(X,axis = 1)\n",
    "print('The max is obtained in average with:',\n",
    "      np.around(np.max(X,axis = 1).mean(),2) )\n",
    "\n",
    "# Create dictionary from quadkeys to NUTS3 region id's\n",
    "quadkey_NUTS3 = {quadkeys[i] : max_rule[i] for i in range(df_NUTS.shape[0])}\n",
    "\n",
    "# Generate NUTS3 Id's for LSCC nodes\n",
    "NUTS3_id = np.asarray([quadkey_NUTS3[list(lscc.nodes())[i]] for i in range(n_nodes)])\n",
    "print('NUTS3 communities in LSCC:', len(set(NUTS3_id)))\n",
    "\n",
    "# Generate NUTS2 Id's\n",
    "NUTS2_id = np.asarray([ NUTS2_index[to_NUTS2[NUTS3[NUTS3_id[i]]]] for i in range(n_nodes)])\n",
    "print('NUTS2 communities in LSCC:', len(set(NUTS2_id)))\n",
    "\n",
    "# Generate NUTS1 Id's\n",
    "NUTS1_id = np.asarray([ NUTS1_index[to_NUTS1[NUTS3[NUTS3_id[i]]]] for i in range(n_nodes)])\n",
    "print('NUTS1 communities in LSCC:', len(set(NUTS1_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Import data #\n",
    "###############\n",
    "\n",
    "root_data = '/Users/ignaciosacristanbarba/Documents/M4R/Data'\n",
    "root_results = '/Users/ignaciosacristanbarba/Documents/M4R/Results'\n",
    "\n",
    "df = pd.read_csv(root_results+'/Timeseries/FilteredDataFrame')\n",
    "# Converting the quadkeys to strings\n",
    "df['start_quadkey'] = df['start_quadkey'].astype(str)\n",
    "df['end_quadkey'] = df['end_quadkey'].astype(str)\n",
    "# adding a leading '0' to quadkeys beginning with 3 so it maps on to web mercator\n",
    "df.loc[df['start_quadkey'].str[:1] == '3', 'start_quadkey'] = '0'+df['start_quadkey']\n",
    "df.loc[df['end_quadkey'].str[:1] == '3', 'end_quadkey'] = '0'+df['end_quadkey']\n",
    "\n",
    "# Replace nan by 0\n",
    "df = df.fillna(0)\n",
    "\n",
    "###########################\n",
    "# Get dates of timeseries #\n",
    "###########################\n",
    "\n",
    "# Get start and end dates\n",
    "start_date = df.columns.values[2][:10]\n",
    "end_date = df.columns.values[-1][:10]\n",
    "\n",
    "# Generate DatetimeIndex\n",
    "days = pd.date_range(start=start_date, end=end_date).date\n",
    "n_days = len(days)\n",
    "days_dm = np.asarray([str(days[i])[5:] for i in range(n_days)])\n",
    "days_week = np.asarray(pd.date_range(start=start_date, end=end_date).weekofyear, dtype='int')\n",
    "weeks = np.arange(days_week.min(),days_week.max())\n",
    "\n",
    "# Store lockdown-date\n",
    "lockdown_date = pd.to_datetime('20200324', format='%Y%m%d', errors='ignore')\n",
    "lockdown_date_number = np.argwhere(days == lockdown_date).flatten()[0]\n",
    "\n",
    "# Store second lockdown-date\n",
    "second_lockdown_date = pd.to_datetime('20201105', format='%Y%m%d', errors='ignore')\n",
    "second_lockdown_date_number = np.argwhere(days == second_lockdown_date).flatten()[0]\n",
    "\n",
    "\n",
    "# compute id's for NUTS1, 2 and 3 and insert them in the dataframe\n",
    "df2 = df.copy()\n",
    "\n",
    "NUTS3_id_start = np.asarray([quadkey_NUTS3[df2['start_quadkey'][i]] for i in range(df2.shape[0])])\n",
    "NUTS2_id_start = np.asarray([ NUTS2_index[to_NUTS2[NUTS3[NUTS3_id_start[i]]]] for i in range(df2.shape[0])])\n",
    "NUTS1_id_start = np.asarray([ NUTS1_index[to_NUTS1[NUTS3[NUTS3_id_start[i]]]] for i in range(df2.shape[0])])\n",
    "\n",
    "NUTS3_id_end = np.asarray([quadkey_NUTS3[df2['end_quadkey'][i]] for i in range(df2.shape[0])])\n",
    "NUTS2_id_end = np.asarray([ NUTS2_index[to_NUTS2[NUTS3[NUTS3_id_end[i]]]] for i in range(df2.shape[0])])\n",
    "NUTS1_id_end = np.asarray([ NUTS1_index[to_NUTS1[NUTS3[NUTS3_id_end[i]]]] for i in range(df2.shape[0])])\n",
    "\n",
    "df2.insert(0,'start_NUTS1',NUTS1_id_start)\n",
    "df2.insert(1,'end_NUTS1',NUTS1_id_end)\n",
    "\n",
    "#function to get keys by the values in a dictionary\n",
    "def getKeysByValues(dictOfElements, listOfValues):\n",
    "    key_list = list(dictOfElements.keys())\n",
    "    val_list = list(dictOfElements.values())\n",
    "    listOfKeys = []\n",
    "    for item  in listOfValues:\n",
    "        position = val_list.index(item)\n",
    "        listOfKeys.append(key_list[position])\n",
    "    return  listOfKeys \n",
    "\n",
    "# numbers of NUTS1 and create dictionary\n",
    "NUTS1_no = df2.start_NUTS1.unique()\n",
    "NUTS1_no_dict = dict(zip(NUTS1_no, range(len(NUTS1_no))))\n",
    "\n",
    "start_NUTS1_abbr = getKeysByValues(NUTS1_index,df2['start_NUTS1'])\n",
    "end_NUTS1_abbr = getKeysByValues(NUTS1_index,df2['end_NUTS1'])\n",
    "\n",
    "df2['start_NUTS1'] = np.asarray([ NUTS1_no_dict[df2['start_NUTS1'][i]] for i in range(df2.shape[0])])\n",
    "df2['end_NUTS1'] = np.asarray([ NUTS1_no_dict[df2['end_NUTS1'][i]] for i in range(df2.shape[0])])\n",
    "\n",
    "# dictionary to name\n",
    "NUTS1_abbr_to_name_dict = {\n",
    "    \"UKC\": \"North East\",\n",
    "    \"UKD\": \"North West\",\n",
    "    \"UKE\": \"Yorkshire\",\n",
    "    \"UKF\": \"East Midlands\",\n",
    "    \"UKG\":\"West Midlands\",\n",
    "    \"UKH\":\"East of England\",\n",
    "    \"UKI\":\"London\",\n",
    "    \"UKJ\":\"South East\",\n",
    "    \"UKK\":\"South West\",\n",
    "    \"UKL\":\"Wales\",\n",
    "    \"UKM\":\"Scotland\",\n",
    "    \"UKN\":\"Northern Ireland\",\n",
    "}\n",
    "\n",
    "# start and end names of NUTS1 regions\n",
    "start_NUTS1_name = [NUTS1_abbr_to_name_dict[start_NUTS1_abbr[i]] for i in range(len(start_NUTS1_abbr))]\n",
    "end_NUTS1_name = [NUTS1_abbr_to_name_dict[end_NUTS1_abbr[i]] for i in range(len(end_NUTS1_abbr))]\n",
    "\n",
    "# create different dataframes for each NUTS1\n",
    "dfs_NUTS1 = []\n",
    "NUTS1_regions_indices = []\n",
    "for i in range(len(NUTS1_no_dict)):\n",
    "    #both are start and end are inside that region\n",
    "    df_tmp = df2.loc[(df2['start_NUTS1'] == i) & (df2['end_NUTS1'] == i)]\n",
    "    dfs_NUTS1.append(df_tmp)\n",
    "    NUTS1_regions_indices.append(df_tmp.index)\n",
    "\n",
    "NUTS1_numbers = getKeysByValues(NUTS1_no_dict,list(NUTS1_no_dict.values()))\n",
    "NUTS1_abbr = getKeysByValues(NUTS1_index,NUTS1_numbers)\n",
    "NUTS1_names = [NUTS1_abbr_to_name_dict[NUTS1_abbr[i]] for i in range(len(NUTS1_abbr))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each dataframe (region), compute lscc base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:120: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "root_results = '/Users/ignaciosacristanbarba/Documents/M4R/Results'\n",
    "root_base = root_results+'/NUTS1_Base/'\n",
    "\n",
    "NUTS1_networks_lscc = []\n",
    "NUTS1_D_geom = []\n",
    "NUTS1_geom_dict = []\n",
    "\n",
    "for i,df in enumerate(dfs_NUTS1):\n",
    "    df2 = df.copy()\n",
    "    # Converting the quadkeys to strings\n",
    "    df2['start_quadkey'] = df['start_quadkey'].astype(str)\n",
    "    df2['end_quadkey'] = df['end_quadkey'].astype(str)\n",
    "    # adding a leading '0' to quadkeys beginning with 3 so it maps on to web mercator\n",
    "    df2.loc[df2['start_quadkey'].str[:1] == '3', 'start_quadkey'] = '0'+df['start_quadkey']\n",
    "    df2.loc[df2['end_quadkey'].str[:1] == '3', 'end_quadkey'] = '0'+df['end_quadkey']\n",
    "\n",
    "    df = df2\n",
    "    # Replace nan by 0\n",
    "    df = df.fillna(0)\n",
    "\n",
    "\n",
    "    ##################\n",
    "    # Aggregate data #\n",
    "    ##################\n",
    "\n",
    "    df = df.reset_index()\n",
    "       \n",
    "    # Get the quadkey columns\n",
    "    df_key = df.iloc[:,3:5]\n",
    "\n",
    "    M = df.shape[1]-3\n",
    "    days = []\n",
    "\n",
    "    for i in range(1,M,3):\n",
    "\n",
    "        df_mov = df.iloc[:,i+1:i+4]\n",
    "        df_sum = pd.Series(df_mov.sum(axis=1),name='movement')\n",
    "        days.append(df_sum)\n",
    "\n",
    "    # Compute weekly average\n",
    "    all_days = days[0]\n",
    "\n",
    "    for i in range(1,len(days)):\n",
    "        all_days += days[i]\n",
    "\n",
    "    all_days = all_days/len(days)\n",
    "    df_mean = pd.concat([df_key,all_days],axis=1)\n",
    "\n",
    "    ####################\n",
    "    # Generate DiGraph #\n",
    "    ####################\n",
    "\n",
    "    N = len(df_mean)\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # for each row, add nodes and weighted edge\n",
    "    for i in range(0,N):\n",
    "        start = df_mean['start_quadkey'][i]\n",
    "        end = df_mean['end_quadkey'][i]\n",
    "        weight = df_mean['movement'][i]\n",
    "        G.add_node(start)\n",
    "        G.add_node(end)\n",
    "        if weight > 0.0:\n",
    "            G.add_weighted_edges_from([(start, end, weight)] )\n",
    "\n",
    "    ###############\n",
    "    # Store Graph #\n",
    "    ###############\n",
    "    G_dict = nx.to_dict_of_dicts(G)\n",
    "    \n",
    "    # Create subgraph of LSCC and save adjacency matrix\n",
    "    lscc_nodes = max(nx.strongly_connected_components(G), key=len)\n",
    "    lscc = G.subgraph(lscc_nodes)\n",
    "    lscc_dict = nx.to_dict_of_dicts(lscc)\n",
    "    \n",
    "    NUTS1_networks_lscc.append(lscc_dict)\n",
    "    \n",
    "    \n",
    "    ###################################\n",
    "    \n",
    "    ####################################\n",
    "    # Compute geometric node positions #\n",
    "    ####################################\n",
    "\n",
    "    # Get polygons from quadkeys\n",
    "    quadkeys = list(G.nodes)\n",
    "    n_nodes = len(quadkeys)\n",
    "    polys = []\n",
    "\n",
    "    #Iterates over the quadkeys to extract the tiles\n",
    "    for quadkey in quadkeys:\n",
    "        tile = mercantile.feature(mercantile.quadkey_to_tile(quadkey), projected = 'web mercator')\n",
    "        polys.append(tile.get('geometry'))\n",
    "    geom = [shape(i) for i in polys]\n",
    "\n",
    "    #  add polygon centroids as node attributes \n",
    "    geom_dict = {quadkeys[i] : list(geom[i].centroid.bounds[:2]) for i in range(0,n_nodes)}\n",
    "    nx.set_node_attributes(G,geom_dict,'geom')\n",
    "\n",
    "    # Save geom_dict to file\n",
    "    NUTS1_geom_dict.append(geom_dict)\n",
    "    # Store node numbers\n",
    "    node_numbers = {i : list(G.nodes())[i] for i in range(n_nodes)}\n",
    "    node_dict = {list(G.nodes)[i] : i for i in range(n_nodes)}\n",
    "\n",
    "    # Gemo dict for node keys\n",
    "    geom_dict_numbers = {i : geom_dict[node_numbers[i]] for i in range(n_nodes)}\n",
    "\n",
    "    # Store node attributes in data frame\n",
    "    node_attributes = pd.DataFrame({'key' : quadkeys, 'geom' : geom_dict.values()})\n",
    "\n",
    "    ###################################\n",
    "    # Compute distances between tiles #\n",
    "    ###################################\n",
    "\n",
    "    gdf = gpd.GeoDataFrame({'geometry':geom, 'quadkey':quadkeys}, crs = \"EPSG:4326\")\n",
    "\n",
    "    def getXY(pt):\n",
    "        return (pt.x, pt.y)\n",
    "    centroidseries = gdf['geometry'].centroid\n",
    "    gdf['LONGITUDE'],gdf['LATITUDE'] = [list(t) for t in zip(*map(getXY, centroidseries))]\n",
    "\n",
    "    from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "    pt_lat = dict(zip(gdf['quadkey'], gdf['LATITUDE']))\n",
    "    pt_long = dict(zip(gdf['quadkey'], gdf['LONGITUDE']))\n",
    "\n",
    "    pt_lat_r = {k: math.radians(v) for k, v in pt_lat.items()}\n",
    "    pt_long_r = {k: math.radians(v) for k, v in pt_long.items()}\n",
    "\n",
    "    pt_tup_1 = []\n",
    "    pt_tup_2 = []\n",
    "    dist_tup = []\n",
    "\n",
    "    # Compute distance in km with Haversine formula\n",
    "    for i in pt_lat_r.keys():\n",
    "        for j in pt_lat_r.keys():\n",
    "            pt_tup_1.append(i)\n",
    "            pt_tup_2.append(j)        \n",
    "            dist_tup.append(6367 * (2 * asin(sqrt(sin((pt_lat_r.get(i) - pt_lat_r.get(j))/2)**2 + cos((pt_lat_r.get(i))) * cos((pt_lat_r.get(j))) * sin((pt_long_r.get(i) - pt_long_r.get(j))/2)**2))))\n",
    "\n",
    "    #Creates Dataframe from tuple\n",
    "    Dist = pd.DataFrame({'start_quadkey': pt_tup_1,'end_quadkey': pt_tup_2,'DISTANCE': dist_tup})\n",
    "\n",
    "    # Compute distance matrix D_geom\n",
    "    D_geom = np.zeros((n_nodes,n_nodes))\n",
    "    for i in range(n_nodes):\n",
    "        D_geom[i,:] = Dist['DISTANCE'][i*n_nodes:(i+1)*n_nodes]\n",
    "        \n",
    "    NUTS1_D_geom.append(D_geom)\n",
    "\n",
    "\n",
    "with open(root_base+'NUTS1_base_geom_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(NUTS1_geom_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(root_base+'/NUTS1_base_D_geom.pickle', 'wb') as handle:\n",
    "    pickle.dump(NUTS1_D_geom, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(root_base+'/NUTS1_base_networks_lscc.npz', 'wb') as handle:\n",
    "    pickle.dump(NUTS1_networks_lscc, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NUTS1_networks_lscc[11])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
